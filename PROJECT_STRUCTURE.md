# LLM UI - Complete Project Structure

## ğŸ“ Project Layout

```
llm-ui-app/
â”œâ”€â”€ README.md                          # Main documentation
â”œâ”€â”€ QUICKSTART.md                      # 5-minute setup guide
â”œâ”€â”€ DEVELOPMENT.md                     # Customization guide
â”œâ”€â”€ SEARXNG_INTEGRATION.md             # Guide for your search tool
â”œâ”€â”€ LLM_MODELS_REFERENCE.md            # LLM configuration guide
â”œâ”€â”€ .env.example                       # Environment variables template
â”œâ”€â”€ run.py                             # Application launcher
â”‚
â”œâ”€â”€ backend/                           # Python FastAPI backend
â”‚   â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚   â”œâ”€â”€ config.py                      # Configuration settings
â”‚   â”œâ”€â”€ settings.py                    # Settings management
â”‚   â”‚
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â””â”€â”€ main.py                    # FastAPI application & SSE streaming
â”‚   â”‚
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py                  # SQLAlchemy models (Conversation, Message, etc.)
â”‚   â”‚   â””â”€â”€ crud.py                    # Database operations
â”‚   â”‚
â”‚   â”œâ”€â”€ mcp_client/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ client.py                  # MCP protocol client & server management
â”‚   â”‚
â”‚   â”œâ”€â”€ llm_client/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ client.py                  # llama.cpp API client
â”‚   â”‚
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base.py                    # Shared utilities (embeddings, reranking)
â”‚       â”œâ”€â”€ rag_service.py             # RAG (Retrieval-Augmented Generation)
â”‚       â”œâ”€â”€ searxng_tool.py            # SearXNG web search integration
â”‚       â”œâ”€â”€ tool_executor.py           # Tool execution with progress tracking
â”‚       â””â”€â”€ tts_service.py             # Text-to-Speech service
â”‚
â””â”€â”€ frontend/                          # HTML/CSS/JS frontend
    â”œâ”€â”€ templates/
    â”‚   â””â”€â”€ index.html                 # Main UI (HTMX + Alpine.js)
    â”‚
    â””â”€â”€ static/
        â”œâ”€â”€ css/
        â”‚   â””â”€â”€ styles.css             # Custom styles
        â”‚
        â””â”€â”€ js/
            â””â”€â”€ app.js                 # Alpine.js application logic
```

## ğŸ“„ Key Files Explained

| File | Purpose | Key Features |
|------|---------|--------------|
| `app/main.py` | Main FastAPI app | â€¢ SSE streaming endpoints<br>â€¢ Conversation management<br>â€¢ MCP server configuration<br>â€¢ Real-time updates<br>â€¢ RAG & web search integration<br>â€¢ Model selection<br>â€¢ Settings management |
| `database/models.py` | Database schema | â€¢ Conversations table<br>â€¢ Messages table<br>â€¢ MCP servers table<br>â€¢ Documents table<br>â€¢ Document chunks table<br>â€¢ Document embeddings table |
| `database/crud.py` | Database operations | â€¢ Async SQLAlchemy queries<br>â€¢ CRUD functions<br>â€¢ Relationship handling |
| `mcp_client/client.py` | MCP integration | â€¢ Multi-server management<br>â€¢ Tool discovery<br>â€¢ Stdio communication<br>â€¢ Dynamic tool loading |
| `llm_client/client.py` | llama.cpp client | â€¢ Streaming chat completion<br>â€¢ OpenAI-compatible API<br>â€¢ Tool integration<br>â€¢ Title generation<br>â€¢ Model switching |
| `tools/tool_executor.py` | Tool execution | â€¢ Progress tracking<br>â€¢ Custom tools support<br>â€¢ MCP tool wrapping<br>â€¢ Error handling<br>â€¢ TTS service integration |
| `tools/searxng_tool.py` | Web search | â€¢ SearXNG integration<br>â€¢ Multi-query generation<br>â€¢ Content extraction<br>â€¢ Semantic reranking<br>â€¢ Citation support |
| `tools/rag_service.py` | RAG service | â€¢ Document processing<br>â€¢ Chunking and indexing<br>â€¢ Semantic search<br>â€¢ Embedding generation<br>â€¢ Re-ranking |
| `tools/tts_service.py` | TTS service | â€¢ Multiple TTS engines<br>â€¢ Edge TTS support<br>â€¢ Offline TTS support<br>â€¢ Audio generation |
| `tools/base.py` | Shared utilities | â€¢ Embedding utilities<br>â€¢ Reranking utilities<br>â€¢ Cosine similarity |
| `config.py` | Configuration | â€¢ Environment variables<br>â€¢ Default settings<br>â€¢ URL configurations |
| `settings.py` | Settings management | â€¢ Runtime settings<br>â€¢ TTS configuration<br>â€¢ Model settings<br>â€¢ UI settings |

### Frontend Files

| File | Purpose | Key Features |
|------|---------|--------------|
| `templates/index.html` | Main UI | â€¢ Chat interface<br>â€¢ Conversation sidebar<br>â€¢ MCP server management<br>â€¢ Real-time updates<br>â€¢ Knowledge base<br>â€¢ Settings modal<br>â€¢ Model selection<br>â€¢ Tool toggles (web search, RAG)<br>â€¢ TTS controls |
| `static/js/app.js` | Application logic | â€¢ Alpine.js reactive state<br>â€¢ SSE event handling<br>â€¢ Message management<br>â€¢ Tool progress display<br>â€¢ TTS integration<br>â€¢ Document management<br>â€¢ Settings management |
| `static/css/styles.css` | Styling | â€¢ Custom scrollbars<br>â€¢ Animations<br>â€¢ Markdown rendering<br>â€¢ Dark mode support<br>â€¢ Responsive design |

## ğŸ”§ Technology Stack

### Backend
- **FastAPI** - Modern Python web framework
- **SQLAlchemy** - ORM with async support
- **aiosqlite** - Async SQLite driver
- **aiohttp** - Async HTTP client
- **Uvicorn** - ASGI server
- **NumPy** - Numerical computations for embeddings
- **BeautifulSoup4** - HTML parsing for web search
- **Requests** - HTTP requests
- **PyPDF2** - PDF processing
- **python-docx** - DOCX processing

### Frontend
- **HTMX** - Dynamic HTML updates
- **Alpine.js** - Reactive UI framework
- **Tailwind CSS** - Utility-first CSS
- **Marked.js** - Markdown rendering

### External Services
- **llama.cpp** - LLM inference engine
- **MCP Servers** - Tool providers via MCP protocol
- **SearXNG** - Privacy-focused web search
- **Edge TTS** - High-quality text-to-speech

## ğŸ¯ Core Features Implementation

### 1. Real-time Chat Streaming
```
User types message
    â†“
POST /api/conversations/{id}/messages
    â†“
Backend creates request_id
    â†“
Frontend connects to SSE: /api/stream/{request_id}
    â†“
Backend streams events (content, thinking, tool_progress, etc.)
    â†“
Frontend updates UI in real-time
```

### 2. MCP Server Integration
```
User adds MCP server via UI
    â†“
Saved to database
    â†“
MCP client starts server process
    â†“
Discovers available tools via MCP protocol
    â†“
Tools available for LLM to use
```

### 3. Tool Execution with Progress
```
LLM decides to use tool
    â†“
Tool executor checks if custom or MCP tool
    â†“
Executes tool with progress callbacks
    â†“
Yields progress events (0-100%)
    â†“
SSE streams progress to frontend
    â†“
UI shows live status updates
```

### 4. Web Search with SearXNG
```
User enables web search
    â†“
Multi-query generation
    â†“
SearXNG search execution
    â†“
Content extraction from results
    â†“
Semantic chunking and embedding
    â†“
Similarity filtering
    â†“
Re-ranking for relevance
    â†“
Citation formatting
    â†“
Context injection for LLM
```

### 5. RAG (Retrieval-Augmented Generation)
```
User uploads document
    â†“
Document processing pipeline
    â†“
Text extraction and chunking
    â†“
Embedding generation for chunks
    â†“
Storage in SQLite with embeddings
    â†“
Semantic search when querying
    â†“
Re-ranking for relevance
    â†“
Context injection for LLM
```

### 6. Persistent Conversations
```
SQLite Database Schema:
    conversations (id, title, created_at, updated_at)
        â†“
    messages (id, conversation_id, role, content, tool_calls, thinking, created_at)
        â†“
    documents (id, filename, filepath, file_type, size_bytes, status, metadata, created_at)
        â†“
    document_chunks (id, document_id, chunk_index, content, start_char, end_char, created_at)
        â†“
    document_embeddings (chunk_id, embedding)
```

## ğŸ“Š Data Flow Diagrams

### Message Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser â”‚â”€â”€â”€â”€â”€â–¶â”‚ FastAPI  â”‚â”€â”€â”€â”€â”€â–¶â”‚ llama.cpp â”‚â”€â”€â”€â”€â”€â–¶â”‚   MCP    â”‚
â”‚ (HTMX)  â”‚â—€â”€â”€â”€â”€â”€â”‚   (SSE)  â”‚â—€â”€â”€â”€â”€â”€â”‚  (Stream) â”‚â—€â”€â”€â”€â”€â”€â”‚  Server  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                 â”‚                                      â”‚
     â”‚                 â–¼                                      â”‚
     â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
     â”‚           â”‚ Database â”‚                                â”‚
     â”‚           â”‚ (SQLite) â”‚                                â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Web Search Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser â”‚â”€â”€â”€â”€â”€â–¶â”‚ FastAPI  â”‚â”€â”€â”€â”€â”€â–¶â”‚ SearXNG  â”‚â”€â”€â”€â”€â”€â–¶â”‚   Web    â”‚
â”‚ (Query) â”‚      â”‚ (Search) â”‚      â”‚ (Query)  â”‚      â”‚ (Pages)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                 â”‚                 â”‚                 â”‚
     â”‚                 â–¼                 â–¼                 â–¼
     â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚           â”‚  Query   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Pages   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Content â”‚
     â”‚           â”‚ Gen      â”‚      â”‚ Extract  â”‚      â”‚ Process  â”‚
     â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                 â”‚                 â”‚                 â”‚
     â”‚                 â–¼                 â–¼                 â–¼
     â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚           â”‚ Embeddingâ”‚â”€â”€â”€â”€â”€â–¶â”‚ Rerank   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Format  â”‚
     â”‚           â”‚ Gen      â”‚      â”‚ Results  â”‚      â”‚ Results  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RAG Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser â”‚â”€â”€â”€â”€â”€â–¶â”‚ FastAPI  â”‚â”€â”€â”€â”€â”€â–¶â”‚  RAG     â”‚â”€â”€â”€â”€â”€â–¶â”‚  SQLite  â”‚
â”‚ (Upload)â”‚      â”‚(Process) â”‚      â”‚ (Index)  â”‚      â”‚ (Chunks) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                 â”‚                 â”‚                 â”‚
     â”‚                 â–¼                 â–¼                 â–¼
     â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚           â”‚  Extract â”‚â”€â”€â”€â”€â”€â–¶â”‚  Chunk   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Store   â”‚
     â”‚           â”‚  Text    â”‚      â”‚  & Emb   â”‚      â”‚  & Link  â”‚
     â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                 â”‚                 â”‚                 â”‚
     â”‚                 â–¼                 â–¼                 â–¼
     â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚           â”‚  Query   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Search  â”‚â”€â”€â”€â”€â”€â–¶â”‚  Retrieveâ”‚
     â”‚           â”‚  Embed   â”‚      â”‚  & Rank  â”‚      â”‚  & Formatâ”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Getting Started Paths

### Path 1: Quick Start (Basic Chat)
1. Start llama.cpp
2. Install Python deps
3. Run application
4. Chat immediately

Time: **5 minutes**
Complexity: **Easy**

### Path 2: With MCP Servers
1. Complete Quick Start
2. Install Node.js
3. Add MCP server via UI
4. Use tools in chat

Time: **10 minutes**
Complexity: **Medium**

### Path 3: Full Integration (Enhanced Features)
1. Complete Path 2
2. Set up SearXNG for web search
3. Configure embedding/reranking models
4. Enable document processing
5. Configure TTS services

Time: **1-2 hours**
Complexity: **Advanced**

## ğŸ“š Documentation Index

| Document | When to Use |
|----------|-------------|
| `README.md` | Overview, features, installation |
| `QUICKSTART.md` | First-time setup in 5 minutes |
| `SEARXNG_INTEGRATION.md` | Integrating your search tool |
| `DEVELOPMENT.md` | Customization and extension |
| `LLM_MODELS_REFERENCE.md` | Model configuration and optimization |

## ğŸ” Common Use Cases

### Use Case 1: Basic Chat Assistant
**What you get:** Clean UI for chatting with local LLM
**Setup needed:** llama.cpp + this app
**Time to value:** 5 minutes

### Use Case 2: Multi-Tool Assistant
**What you get:** LLM with access to filesystem, time, etc.
**Setup needed:** llama.cpp + this app + MCP servers
**Time to value:** 15 minutes

### Use Case 3: Research Assistant (Enhanced)
**What you get:** LLM + web search with citations + document analysis + TTS
**Setup needed:** Full integration with SearXNG + embedding + reranking + TTS
**Time to value:** 2-3 hours

## ğŸ¨ Customization Points

### Easy Customizations
- âœ… Change UI colors/theme
- âœ… Modify conversation title length
- âœ… Add custom CSS animations
- âœ… Change default settings
- âœ… Adjust model parameters (temp, max_tokens)

### Medium Customizations
- âš™ï¸ Add document upload
- âš™ï¸ Implement user authentication
- âš™ï¸ Add conversation export
- âš™ï¸ Custom system prompts
- âš™ï¸ Configure TTS voices and settings

### Advanced Customizations
- ğŸ”§ Multi-modal support
- ğŸ”§ Custom embedding pipeline
- ğŸ”§ Advanced tool orchestration
- ğŸ”§ Custom search algorithms
- ğŸ”§ Vector database integration

## ğŸ› Troubleshooting Quick Reference

| Issue | Solution File | Section |
|-------|---------------|---------|
| Can't connect to llama.cpp | `QUICKSTART.md` | Troubleshooting |
| MCP server won't start | `README.md` | MCP Server Setup |
| No progress updates | `DEVELOPMENT.md` | Streaming with Tool Results |
| Search tool integration | `SEARXNG_INTEGRATION.md` | Integration Steps |
| Customizing UI | `DEVELOPMENT.md` | Change UI Theme/Colors |
| TTS not working | `QUICKSTART.md` | TTS troubleshooting |
| Document processing fails | `DEVELOPMENT.md` | Document Processing |

## ğŸ“ˆ Performance Characteristics

### Expected Latency
- Initial page load: **< 1s**
- Message send: **< 100ms**
- LLM first token: **200-500ms** (depends on model)
- Tool execution: **1-10s** (depends on tool)
- Web search: **5-15s** (depends on results)
- Document processing: **10-60s** (depends on size)
- Database query: **< 50ms**

### Scalability
- **Current:** Single-user, local deployment
- **Potential:** Multi-user with PostgreSQL + Redis
- **Bottleneck:** llama.cpp inference speed

## ğŸ” Security Considerations

### Current State (Local Use)
- âœ… No authentication (single user)
- âœ… Local database
- âœ… No external API keys exposed by default

### Production Recommendations
- ğŸ”’ Add JWT authentication
- ğŸ”’ Use HTTPS
- ğŸ”’ Validate MCP server inputs
- ğŸ”’ Rate limiting
- ğŸ”’ Input sanitization
- ğŸ”’ Secure file uploads
- ğŸ”’ Environment variable management

## ğŸ¯ Next Steps After Setup

1. âœ… **Complete QUICKSTART.md** - Get basic chat working
2. âœ… **Add one MCP server** - Test tool integration
3. âœ… **Explore enhanced features** - Web search, RAG, TTS
4. âœ… **Customize UI** - Make it yours
5. âœ… **Configure models** - Optimize for your use case

## ğŸ’¡ Tips for Success

### For First-Time Setup
- Start with the simplest config
- Test llama.cpp separately first
- Use small model for testing
- Check logs frequently

### For Development
- Enable DEBUG mode
- Use browser DevTools
- Test tools independently
- Read the source code

### For Integration
- Start with one custom tool
- Test progress updates thoroughly
- Handle errors gracefully
- Document your changes

## ğŸ¤ Community & Support

While this is a standalone project, here are resources:

- **MCP Protocol:** https://modelcontextprotocol.io/
- **llama.cpp:** https://github.com/ggerganov/llama.cpp
- **FastAPI:** https://fastapi.tiangolo.com/
- **Alpine.js:** https://alpinejs.dev/
- **SearXNG:** https://searxng.org/
- **Edge TTS:** https://github.com/rany2/edge-tts

## ğŸ“ License

MIT License - Use freely for personal or commercial projects!

---

**Ready to start?** â†’ Open `QUICKSTART.md`

**Need help?** â†’ Check the appropriate guide above

**Want to customize?** â†’ See `DEVELOPMENT.md`

**Model configuration?** â†’ See `LLM_MODELS_REFERENCE.md`